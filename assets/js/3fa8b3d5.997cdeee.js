"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[1622],{5818:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>r,contentTitle:()=>a,default:()=>_,frontMatter:()=>s,metadata:()=>l,toc:()=>c});var o=i(5893),t=i(1151);const s={custom_edit_url:"https://github.com/microsoft/autogen/edit/main/website/docs/llm_endpoint_configuration.ipynb"},a="LLM Endpoint Configuration",l={id:"llm_endpoint_configuration",title:"LLM Endpoint Configuration",description:"TL;DR",source:"@site/docs/llm_endpoint_configuration.mdx",sourceDirName:".",slug:"/llm_endpoint_configuration",permalink:"/autogen/docs/llm_endpoint_configuration",draft:!1,unlisted:!1,editUrl:"https://github.com/microsoft/autogen/edit/main/website/docs/llm_endpoint_configuration.ipynb",tags:[],version:"current",frontMatter:{custom_edit_url:"https://github.com/microsoft/autogen/edit/main/website/docs/llm_endpoint_configuration.ipynb"},sidebar:"docsSidebar",previous:{title:"Optional Dependencies",permalink:"/autogen/docs/installation/Optional-Dependencies"},next:{title:"Multi-agent Conversation Framework",permalink:"/autogen/docs/Use-Cases/agent_chat"}},r={},c=[{value:"TL;DR",id:"tldr",level:2},{value:"In-depth",id:"in-depth",level:2},{value:"Steps:",id:"steps",level:3},{value:"What is a <code>config_list</code>?",id:"what-is-a-config_list",level:3},{value:"get_config_list",id:"get_config_list",level:2},{value:"config_list_openai_aoai",id:"config_list_openai_aoai",level:2},{value:"config_list_from_json",id:"config_list_from_json",level:2},{value:"What is <code>filter_dict</code>?",id:"what-is-filter_dict",level:4},{value:"config_list_from_models",id:"config_list_from_models",level:2},{value:"config_list_from_dotenv",id:"config_list_from_dotenv",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,t.a)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h1,{id:"llm-endpoint-configuration",children:"LLM Endpoint Configuration"}),"\n",(0,o.jsx)(n.h2,{id:"tldr",children:"TL;DR"}),"\n",(0,o.jsx)(n.p,{children:"For just getting started with AutoGen you can use the following to\ndefine your LLM endpoint configuration:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import autogen\n\nconfig_list = [{"model": "gpt-4", "api_key": "YOUR_OPENAI_API_KEY"}]\n'})}),"\n",(0,o.jsx)(n.admonition,{type:"danger",children:(0,o.jsx)(n.p,{children:"Never commit secrets into your code. Before committing, change the code\nto use a different way of providing your API keys as described below."})}),"\n",(0,o.jsx)(n.h2,{id:"in-depth",children:"In-depth"}),"\n",(0,o.jsxs)(n.p,{children:["Managing API configurations can be tricky, especially when dealing with\nmultiple models and API versions. The provided utility functions assist\nusers in managing these configurations effectively. Ensure your API keys\nand other sensitive data are stored securely. You might store keys in\n",(0,o.jsx)(n.code,{children:".txt"})," or ",(0,o.jsx)(n.code,{children:".env"})," files or environment variables for local development.\nNever expose your API keys publicly. If you insist on storing your key\nfiles locally on your repo (you shouldn\u2019t), ensure the key file path is\nadded to the ",(0,o.jsx)(n.code,{children:".gitignore"})," file."]}),"\n",(0,o.jsx)(n.h3,{id:"steps",children:"Steps:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Obtain API keys from OpenAI and optionally from Azure OpenAI (or\nother provider)."}),"\n",(0,o.jsxs)(n.li,{children:["Store them securely using either:","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["Environment Variables: ",(0,o.jsx)(n.code,{children:"export OPENAI_API_KEY='your-key'"})," in\nyour shell."]}),"\n",(0,o.jsxs)(n.li,{children:["Text File: Save the key in a ",(0,o.jsx)(n.code,{children:"key_openai.txt"})," file."]}),"\n",(0,o.jsxs)(n.li,{children:["Env File: Save the key to a ",(0,o.jsx)(n.code,{children:".env"})," file eg:\n",(0,o.jsx)(n.code,{children:"OPENAI_API_KEY=sk-********************"})]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsxs)(n.a,{href:"/autogen/docs/installation/",children:["Ensure ",(0,o.jsx)(n.code,{children:"pyautogen"})," is installed"]})}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["There are many ways to generate a ",(0,o.jsx)(n.code,{children:"config_list"})," depending on your use\ncase:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"get_config_list"}),": Generates configurations for API calls, primarily\nfrom provided API keys."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"config_list_openai_aoai"}),": Constructs a list of configurations using\nboth Azure OpenAI and OpenAI endpoints, sourcing API keys from\nenvironment variables or local files."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"config_list_from_json"}),": Loads configurations from a JSON structure,\neither from an environment variable or a local JSON file, with the\nflexibility of filtering configurations based on given criteria."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"config_list_from_models"}),": Creates configurations based on a\nprovided list of models, useful when targeting specific models\nwithout manually specifying each configuration."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"config_list_from_dotenv"}),": Constructs a configuration list from a\n",(0,o.jsx)(n.code,{children:".env"})," file, offering a consolidated way to manage multiple API\nconfigurations and keys from a single file."]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["If multiple models are provided, the Autogen client (",(0,o.jsx)(n.code,{children:"OpenAIWrapper"}),")\nand agents don\u2019t choose the \u201cbest model\u201d on any criteria - inference is\ndone through the very first model and the next one is used only if the\ncurrent model fails (e.g.\xa0API throttling by the provider or a filter\ncondition is unsatisfied)."]}),"\n",(0,o.jsxs)(n.h3,{id:"what-is-a-config_list",children:["What is a ",(0,o.jsx)(n.code,{children:"config_list"}),"?"]}),"\n",(0,o.jsxs)(n.p,{children:["When instantiating an assistant, such as the example below, it is passed\na ",(0,o.jsx)(n.code,{children:"config_list"}),". This is used to tell the ",(0,o.jsx)(n.code,{children:"AssistantAgent"})," which models\nor configurations it has access to:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'\nassistant = AssistantAgent(\n    name="assistant",\n    llm_config={\n        "timeout": 600,\n        "cache_seed": 42,\n        "config_list": config_list,\n        "temperature": 0,\n    },\n)\n'})}),"\n",(0,o.jsx)(n.p,{children:"Consider an intelligent assistant that utilizes OpenAI\u2019s GPT models.\nDepending on user requests, it might need to:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Generate creative content (using gpt-4)."}),"\n",(0,o.jsx)(n.li,{children:"Answer general queries (using gpt-3.5-turbo)."}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["Different tasks may require different models, and the ",(0,o.jsx)(n.code,{children:"config_list"})," aids\nin dynamically selecting the appropriate model configuration, managing\nAPI keys, endpoints, and versions for efficient operation of the\nintelligent assistant. In summary, the ",(0,o.jsx)(n.code,{children:"config_list"})," helps the agents\nwork efficiently, reliably, and optimally by managing various\nconfigurations and interactions with the OpenAI API - enhancing the\nadaptability and functionality of the agents."]}),"\n",(0,o.jsx)(n.h2,{id:"get_config_list",children:"get_config_list"}),"\n",(0,o.jsx)(n.p,{children:"Used to generate configurations for API calls."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'api_keys = ["YOUR_OPENAI_API_KEY"]\nbase_urls = None  # You can specify API base URLs if needed. eg: localhost:8000\napi_type = "openai"  # Type of API, e.g., "openai" or "aoai".\napi_version = None  # Specify API version if needed.\n\nconfig_list = autogen.get_config_list(api_keys, base_urls=base_urls, api_type=api_type, api_version=api_version)\n\nprint(config_list)\n'})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-text",children:"[{'api_key': 'YOUR_OPENAI_API_KEY', 'api_type': 'openai'}]\n"})}),"\n",(0,o.jsx)(n.h2,{id:"config_list_openai_aoai",children:"config_list_openai_aoai"}),"\n",(0,o.jsx)(n.p,{children:"This method creates a list of configurations using Azure OpenAI\nendpoints and OpenAI endpoints. It tries to extract API keys and bases\nfrom environment variables or local text files."}),"\n",(0,o.jsxs)(n.p,{children:["Steps: - Store OpenAI API key in: - Environment variable:\n",(0,o.jsx)(n.code,{children:"OPENAI_API_KEY"})," - or Local file: ",(0,o.jsx)(n.code,{children:"key_openai.txt"})," - Store Azure OpenAI\nAPI key in: - Environment variable: ",(0,o.jsx)(n.code,{children:"AZURE_OPENAI_API_KEY"})," - or Local\nfile: ",(0,o.jsx)(n.code,{children:"key_aoai.txt"})," (Supports multiple keys, one per line) - Store\nAzure OpenAI API base in: - Environment variable:\n",(0,o.jsx)(n.code,{children:"AZURE_OPENAI_API_BASE"})," - or Local file: ",(0,o.jsx)(n.code,{children:"base_aoai.txt"})," (Supports\nmultiple bases, one per line)"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'config_list = autogen.config_list_openai_aoai(\n    key_file_path=".",\n    openai_api_key_file="key_openai.txt",\n    aoai_api_key_file="key_aoai.txt",\n    aoai_api_base_file="base_aoai.txt",\n    exclude=None,  # The API type to exclude, eg: "openai" or "aoai".\n)\n'})}),"\n",(0,o.jsx)(n.h2,{id:"config_list_from_json",children:"config_list_from_json"}),"\n",(0,o.jsx)(n.p,{children:"This method loads configurations from an environment variable or a JSON\nfile. It provides flexibility by allowing users to filter configurations\nbased on certain criteria."}),"\n",(0,o.jsxs)(n.p,{children:["Steps: - Setup the JSON Configuration: 1. Store configurations in an\nenvironment variable named ",(0,o.jsx)(n.code,{children:"OAI_CONFIG_LIST"})," as a valid JSON string. 2.\nAlternatively, save configurations in a local JSON file named\n",(0,o.jsx)(n.code,{children:"OAI_CONFIG_LIST.json"})," 3. Add ",(0,o.jsx)(n.code,{children:"OAI_CONFIG_LIST"})," to your ",(0,o.jsx)(n.code,{children:".gitignore"}),"\nfile on your local repository."]}),"\n",(0,o.jsx)(n.p,{children:"Your JSON structure should look something like this:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-json",children:'# OAI_CONFIG_LIST file example\n[\n    {\n        "model": "gpt-4",\n        "api_key": "YOUR_OPENAI_API_KEY"\n    },\n    {\n        "model": "gpt-3.5-turbo",\n        "api_key": "YOUR_OPENAI_API_KEY",\n        "api_version": "2023-03-01-preview"\n    }\n]\n\n'})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'config_list = autogen.config_list_from_json(\n    env_or_file="OAI_CONFIG_LIST",  # or OAI_CONFIG_LIST.json if file extension is added\n    filter_dict={\n        "model": {\n            "gpt-4",\n            "gpt-3.5-turbo",\n        }\n    },\n)\n'})}),"\n",(0,o.jsxs)(n.h4,{id:"what-is-filter_dict",children:["What is ",(0,o.jsx)(n.code,{children:"filter_dict"}),"?"]}),"\n",(0,o.jsxs)(n.p,{children:["The z parameter in ",(0,o.jsx)(n.code,{children:"autogen.config_list_from_json"})," function is used to\nselectively filter the configurations loaded from the environment\nvariable or JSON file based on specified criteria. It allows you to\ndefine criteria to select only those configurations that match the\ndefined conditions."]}),"\n",(0,o.jsxs)(n.p,{children:["let\u2019s say you want to configure an assistant agent to only LLM type.\nTake the below example: even though we have \u201cgpt-3.5-turbo\u201d and \u201cgpt-4\u201d\nin our ",(0,o.jsx)(n.code,{children:"OAI_CONFIG_LIST"}),", this agent would only be configured to use"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'cheap_config_list = autogen.config_list_from_json(\n    env_or_file="OAI_CONFIG_LIST",\n    filter_dict={\n        "model": {\n            "gpt-3.5-turbo",\n        }\n    },\n)\n\ncostly_config_list = autogen.config_list_from_json(\n    env_or_file="OAI_CONFIG_LIST",\n    filter_dict={\n        "model": {\n            "gpt-4",\n        }\n    },\n)\n\n# Assistant using GPT 3.5 Turbo\nassistant_one = autogen.AssistantAgent(\n    name="3.5-assistant",\n    llm_config={\n        "timeout": 600,\n        "cache_seed": 42,\n        "config_list": cheap_config_list,\n        "temperature": 0,\n    },\n)\n\n# Assistant using GPT 4\nassistant_two = autogen.AssistantAgent(\n    name="4-assistant",\n    llm_config={\n        "timeout": 600,\n        "cache_seed": 42,\n        "config_list": costly_config_list,\n        "temperature": 0,\n    },\n)\n'})}),"\n",(0,o.jsxs)(n.p,{children:["With the ",(0,o.jsx)(n.code,{children:"OAI_CONFIG_LIST"})," we set earlier, there isn\u2019t much to filter\non. But when the complexity of a project grows and you\u2019re managing\nmultiple models for various purposes, you can see how ",(0,o.jsx)(n.code,{children:"filter_dict"})," can\nbe useful."]}),"\n",(0,o.jsxs)(n.p,{children:["A more complex filtering criteria could be the following: Assuming we\nhave an ",(0,o.jsx)(n.code,{children:"OAI_CONFIG_LIST"})," several models used to create various agents -\nLet\u2019s say we want to load configurations for ",(0,o.jsx)(n.code,{children:"gpt-4"})," using API version\n",(0,o.jsx)(n.code,{children:'"2023-03-01-preview"'})," and we want the ",(0,o.jsx)(n.code,{children:"api_type"})," to be ",(0,o.jsx)(n.code,{children:"aoai"}),", we can\nset up ",(0,o.jsx)(n.code,{children:"filter_dict"})," as follows:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'config_list = autogen.config_list_from_json(\n    env_or_file="OAI_CONFIG_LIST",\n    filter_dict={"model": {"gpt-4"}, "api_version": {"2023-03-01-preview"}, "api_type": ["aoai"]},\n)\n'})}),"\n",(0,o.jsx)(n.h2,{id:"config_list_from_models",children:"config_list_from_models"}),"\n",(0,o.jsxs)(n.p,{children:["This method creates configurations based on a provided list of models.\nIt\u2019s useful when you have specific models in mind and don\u2019t want to\nmanually specify each configuration. The\n",(0,o.jsx)(n.a,{href:"../docs/reference/oai/openai_utils#config_list_from_models",children:(0,o.jsx)(n.code,{children:"config_list_from_models"})}),"\nfunction tries to create a list of configurations using Azure OpenAI\nendpoints and OpenAI endpoints for the provided list of models. It\nassumes the api keys and api bases are stored in the corresponding\nenvironment variables or local txt files. It\u2019s okay to only have the\nOpenAI API key, OR only the Azure OpenAI API key + base. For Azure the\nmodel name refers to the OpenAI Studio deployment name."]}),"\n",(0,o.jsxs)(n.p,{children:["Steps: - Similar to method 1, store API keys and bases either in\nenvironment variables or ",(0,o.jsx)(n.code,{children:".txt"})," files."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'config_list = autogen.config_list_from_models(\n    key_file_path=".",\n    openai_api_key_file="key_openai.txt",\n    aoai_api_key_file="key_aoai.txt",\n    aoai_api_base_file="base_aoai.txt",\n    exclude="aoai",\n    model_list=["gpt-4", "gpt-3.5-turbo", "gpt-3.5-turbo-16k"],\n)\n'})}),"\n",(0,o.jsx)(n.h2,{id:"config_list_from_dotenv",children:"config_list_from_dotenv"}),"\n",(0,o.jsxs)(n.p,{children:["If you are interested in keeping all of your keys in a single location\nlike a ",(0,o.jsx)(n.code,{children:".env"})," file rather than using a configuration specifically for\nOpenAI, you can use ",(0,o.jsx)(n.code,{children:"config_list_from_dotenv"}),". This allows you to\nconveniently create a config list without creating a complex\n",(0,o.jsx)(n.code,{children:"OAI_CONFIG_LIST"})," file."]}),"\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.code,{children:"model_api_key_map"})," parameter is a dictionary that maps model names\nto the environment variable names in the ",(0,o.jsx)(n.code,{children:".env"})," file where their\nrespective API keys are stored. It lets the code know which API key to\nuse for each model."]}),"\n",(0,o.jsxs)(n.p,{children:["If not provided, it defaults to using ",(0,o.jsx)(n.code,{children:"OPENAI_API_KEY"})," for ",(0,o.jsx)(n.code,{children:"gpt-4"})," and\n",(0,o.jsx)(n.code,{children:"OPENAI_API_KEY"})," for ",(0,o.jsx)(n.code,{children:"gpt-3.5-turbo"}),"."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'    # default key map\n    model_api_key_map = {\n        "gpt-4": "OPENAI_API_KEY",\n        "gpt-3.5-turbo": "OPENAI_API_KEY",\n    }\n'})}),"\n",(0,o.jsxs)(n.p,{children:["Here is an example ",(0,o.jsx)(n.code,{children:".env"})," file:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"OPENAI_API_KEY=sk-*********************\nHUGGING_FACE_API_KEY=**************************\nANOTHER_API_KEY=1234567890234567890\n"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'config_list = autogen.config_list_from_dotenv(\n    dotenv_file_path=".env",  # If None the function will try to find in the working directory\n    filter_dict={\n        "model": {\n            "gpt-4",\n            "gpt-3.5-turbo",\n        }\n    },\n)\n\nconfig_list\n'})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-text",children:"[{'api_key': 'sk-*********************', 'model': 'gpt-4'},\n {'api_key': 'sk-*********************', 'model': 'gpt-3.5-turbo'}]\n"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# gpt-3.5-turbo will default to OPENAI_API_KEY\nconfig_list = autogen.config_list_from_dotenv(\n    dotenv_file_path=".env",  # If None the function will try to find in the working directory\n    model_api_key_map={\n        "gpt-4": "ANOTHER_API_KEY",  # String or dict accepted\n    },\n    filter_dict={\n        "model": {\n            "gpt-4",\n            "gpt-3.5-turbo",\n        }\n    },\n)\n\nconfig_list\n'})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-text",children:"[{'api_key': '1234567890234567890', 'model': 'gpt-4'},\n {'api_key': 'sk-*********************', 'model': 'gpt-3.5-turbo'}]\n"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# example using different environment variable names\nconfig_list = autogen.config_list_from_dotenv(\n    dotenv_file_path=".env",\n    model_api_key_map={\n        "gpt-4": "OPENAI_API_KEY",\n        "vicuna": "HUGGING_FACE_API_KEY",\n    },\n    filter_dict={\n        "model": {\n            "gpt-4",\n            "vicuna",\n        }\n    },\n)\n\nconfig_list\n'})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-text",children:"[{'api_key': 'sk-*********************', 'model': 'gpt-4'},\n {'api_key': '**************************', 'model': 'vicuna'}]\n"})}),"\n",(0,o.jsxs)(n.p,{children:["You can also provide additional configurations for APIs, simply by\nreplacing the string value with a dictionary expanding on the\nconfigurations. See the example below showing the example of using\n",(0,o.jsx)(n.code,{children:"gpt-4"})," on ",(0,o.jsx)(n.code,{children:"openai"})," by default, and using ",(0,o.jsx)(n.code,{children:"gpt-3.5-turbo"})," with\nadditional configurations for ",(0,o.jsx)(n.code,{children:"aoai"}),"."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'config_list = autogen.config_list_from_dotenv(\n    dotenv_file_path=".env",\n    model_api_key_map={\n        "gpt-4": "OPENAI_API_KEY",\n        "gpt-3.5-turbo": {\n            "api_key_env_var": "ANOTHER_API_KEY",\n            "api_type": "aoai",\n            "api_version": "v2",\n            "base_url": "https://api.someotherapi.com",\n        },\n    },\n    filter_dict={\n        "model": {\n            "gpt-4",\n            "gpt-3.5-turbo",\n        }\n    },\n)\n\nconfig_list\n'})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-text",children:"[{'api_key': 'sk-*********************', 'model': 'gpt-4'},\n {'api_key': '1234567890234567890',\n  'base_url': 'https://api.someotherapi.com',\n  'api_type': 'aoai',\n  'api_version': 'v2',\n  'model': 'gpt-3.5-turbo'}]\n"})})]})}function _(e={}){const{wrapper:n}={...(0,t.a)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},1151:(e,n,i)=>{i.d(n,{Z:()=>l,a:()=>a});var o=i(7294);const t={},s=o.createContext(t);function a(e){const n=o.useContext(s);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);